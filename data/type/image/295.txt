
Overview
This repository contains the supporting code used on a dataset published as part of the Avito Demand Prediction competition. This dataset is made of a total of more than 16.5M classified ads, out of which only less than 10% are labeled. This classified ad data contains structured data (item price, publication date and period, city, category...), image data and free text data (title, item description). The objective of this problem is to estimate a variable named "deal probability" taking continuous values between 0 and 1 and most likely representing a proxy to the "click-through-rate" for items as a regression variable.
This combination of abundance of unlabeled data, limited labeled data and diverse modalities (structured data, image data and text data) make this problem an interesting application of transfer learning techniques and multi modal fusion. This work presents a proposal solution using deep neural network implemented in PyTorch.
Guidelines
A static configuration file (config.yaml) is used to point to the location of the raw dataset (can be downloaded from the Kaggle website - Avito Demand Prediction), intermediary snapshot files, model parameters and run option (e.g. sample flag to speed up debugging and testing). The main entry point to the application is main.py, where arguments are given to specify the modeling phase (structured, nlp or image) and the phase (pretraining, training, test, actiavtions). Additional optimional arguments (for example --preproc or --no-preproc) control if the preprocessing steps should be re-run, skipped or saved for future re-use.
Approach
The data contains about 30GB of tabular data (including text) and 70GB of image data. The proposed solution is based on a number of deep neural networks for aggregating the structured features with features extracted from text and images, deep language models to generate features from text and an Image Quality Assessment (IQA) estimator. The language and image models are run separately and store their activations/features in a text file. This text file is then used by the structured model (ensembling model) to generate final predictions for the objective variable.

Text model (NLP)
For the text modeling, containing most of the complexity for this project, a 2-step approach is used to leverage the large amount of unlabeled data available. A language model encoder is first trained on the large amount of unlabeled data available. As an alternative to standard word embedding techniques such as continuous bag of word or skip-gram, the full encoder architecture to be used in the final text model is pre-trained allowing transfer learning not only on words representations, but also on the information extraction from classified ad sentences. The encoder pre-trained on the full data is then re-used for a model predicting the target variable, using transfer learning and applying a small learning rate to the encoder layers and a training from scratch for the decoder layers. The activations from this decoder layer (set to 100 for this implementation) are used as features for the structured model.
Language model (pre-training)
The major steps involved in the language model pre-training are shown in the picture below:


Corpus creation: For training the language model all descriptions are aggregated (appended one after the other in a text file. Use of an  tag after each description. The split between the item IDs has only little importance for the language model pre-training as most of the data will be unlabeled anyway. This corpus aggregation step allows the subsequent generation of batches of comparable size allowing efficient parallelization and eliminating the impact of very variable description lengths.
NLP Pre-processing: Two specificities of the text data in this case: a free text format where users selling items use non-standard syntax but also rely on the use of smiley and special characters as a way to format the text (e.g. sequence of special characters as separator such as ============). The second specificity is related to the high morphology of the language of the ads: Russian. If a simple tokenization strategy is applied, the size of the vocabulary row very large because of the different declinations of a same word. The approach used relies on a specialized, industry standard Russian Yandex lemmatizer. A clean-up using regular expressions reduces all sequence of consecutive identical special characters (excl. Cyrillic character set) to 1 to avoid the generation for different tokens for these (an alternative to keep the receptivity information could have been to separate them by a space and have a resulting sequence of identical tokens). All numbers are replaced by a  tag (alternative could have been binning of the numbers). This pre-processing step is fully parallelized which is critical given the size of the input, and allows for serialization of the processed data for subsequent reloading to speed up modeling experimentation.
Dataloading: The corpus is split into 64 batches/chunks. For each batch, iterate and generate sequences of length BPTT (for input) and the same sequence shifted by 1 token for the target (this allows for faster stateful training of the LSTM units). The resulting input tensor size is BSxBPTT starting at BS different locations in corpus, and with a slightly varying BPTT length from iteration to iteration (serving as a data augmentation and regularization effect if several epochs are run through the dataset).
Embedding: Translate vocabulary size (c.a. 50k words) to a tensor of size 200. Followed by a dropout layer.
LSTM-based encoder: Iterate through the input sequences via a deep LSTM (3 layers) with regularization (dropout and batch normalization).
Linear decoder: Temporary layer used only for the pre-training phase and allowing the encoder to output the next word. Large set of weights (output of the same size as the vocabulary used to generate embeddings).

Text model (transfer learning)
This step uses the embedding and LSTM encoder pre-trained in the previous step to generate predictions for the target variable and activations to be used as features for the ensembling model.


Pre-processing: These steps are identical to the language model pre-training, except for the corpus creation. In this phase, the descriptions and texts need to be processed individually as the model needs to output a variable aligned by the target. This also implies this phase will be limited to the labeled data.
Dataloading: In this phase the examples need to be processed independently. The parallelization is improved by grouping the training example by lengths to generate batches of similar size (using a bucket iterator). As some description are outliers in terms of length, a clipping parameter limits the maximum size of a sequence to process. The resulting dataloaders generate inputs of size min(Length, MaxLength) x BS and targets of size BS.
Embedding: the embedding matrix trained on the full unlabeled dataset is reused for this .
LSTM-based encoder: The weights from pre-training on the unlabeled data are re-used. The architecture is modified to repeat the processing on sequences lengths similar to the pre-training. This effectively means that for a description of size 4 times larger than the sequence length of the RNN, 4 outputs will be concatenated and outputted by the encoder.
Max/Avg Pool: Because the encoder outputs a sequence of variable length (given by [text token numbers]//[sequence length]), it needs to be consolidated back to a fixed output size. A max and avg global pool layers are used along the variable dimension and the two resulting outputs are concatenated.
Linear output: This final fully connected layer takes the concatenated pool values and outputs a number between 0 and 1 (target variable) with intermediate layer with 100 units. The activations for the intermediate layer are also returned by a forward pass of the network as they will be used as features by the ensembling model, although they are not used for the loss calculation and back-propagation.

Image model
In order to process the images, several approaches were considered, including for example using pre-trained imageNet models to out a class or activations to be used by an ensembling model. However, the actual category of the item in an image does not convey information that is directly useful to estimate a click-through rate. Instead, the image quality is an independent characteristic of the image that is likely to have a significant impact on the quality of a classified ad. For this step, a series of image-quality related features (for example resolution, contrast, brightness, blur...) are calculated using a high level of parallelization and standard libraries (OpenCV). The features generated are then stored and used as features by the ensembling neural network.

Ensembling model
The ensembling model is a neural network made of a limited number of layers (typically 3) with embedding representation for the categorical variables in the dataset. Because the text and image features are pre-calculated and loaded as part of the dataset, this network is using fully structured data and trains in a limited amount of time. As an alternative, gradient-boosting methods using the same set of generated features have been investigated and perform at similar levels of accuracy, albeit with a slightly longer training time.

