
Million Songs Genre classification
This was an analysis on simple artist/song features and the additional benefit on genre classification from user-generated tags data. The purpose of this project was to utilize classification models, cloud computing services, and PostgreSQL.
Abstract:
Using the Million Song Dataset (MSD), I used 16 song/artist features to baseline a logistic regression classification model on Jazz and Rock music genres. I compared this with a similar model that included 2314 unique terms generated by music listeners that described artists on 3 general categories: nationality, genre, and descriptive terms. In these documents, I prove that this social data significantly improves upon the baseline model for classifying jazz songs by 5.3 and 6.4%. Decision tree methods and KNN were tried but results were not conclusive to include. In the conclusion I provide my suggestion for a pseudo-client.
Data Acquisition:
The MSD was downloaded from labrosa (1). Data was stored in a Postgres database by my colleague Aaron Munoz. I queried this database and stored the data locally in csv format.
Data Description:
The dataset came in 9 relational tables which contained artist level and song level features. Labels and accompanying user-generated tag terms were at an artist level, however analysis was done on a song level for full use of the set. 12 genres were selected  out of 7,643 unique genres and of the 12 (+ ‘other’), 2 genres were used for analysis due to the time constraints while working on the project. The 16 main features were descriptive numerical values like year published, lat/long of artist location, duration of song, artist popularity scores, the tempo of song, etc. I used one hot encoding on the term tags to create a sparse matrix of 2314 columns for the 1 million songs. These term tags were either a location/ nationality, a genre, or a song description like: UK/ British, Rock and Indie, or female vocals, respectively. Downcasting methods were used on the final table to reduce the size from 18.5GB to 2.6GB. I used 99.5%  of the millions songs for analyses and statistical modeling.
Analyses:
There was overlap between genres, most notable folk and country. This presented problems early for multi-classification, therefore I was limited to one vs all methods and I had time to review rock and jazz. Correlations of the main 16 features were fairly weak; correlations were within a range of (-0.3, 0.3).
Model Setup:
I used a train-validate-test (64/16/20) split for statistical modeling. The positive class weights for rock and jazz were 76.7% and 47%, respectively.
Statistical Modeling:
Using a basic logistic regression, I found that rock had significant effects from all main features, the most important being artist familiarity score, where the odds ratio was between (25.23, 27.96) meaning that for a rock song, a very popular artist (score of 1) was about 26 times more likely than a very unpopular artist (score of 0) to be labeled as rock given all other variables (held constant). Song mode and time signature confidence were not significant variables for the jazz regression and had no variables with strong effects akin to artist familiarity in the rock regression.
I tested the differences between various class balancing parameters for the main baseline model and the full model calculating the F1, AUC, and accuracy scores for each model and found that for our desired AUC scoring, the balanced class weighting worked best. The ROC-AUC score was selected for its comparability between differently imbalanced genres and its tradeoff between the true positive rate and false positive rate (how well the models’ positive predictions performed).
I used GridSearchCV to tune hyperparameters for Logistic Regression (normally weighted and balanced class weights, both with lasso regularization) and a Decision Tree model and found that the Decision Tree performed better than Logistic Regression on the lower dimensions but overfit with all features, due to the curse of dimensionality. Confusion plots and ROC curves were used to visualize the results and after validating models I chose Balanced Logistic Regression as my final model for testing on a holdout test set.
Final scoring of the chosen model on the jazz genre, with term tag features, received an F1 score of .602, ROC-AUC score of .6865, and Accuracy score of .6347. Compared to the main features model with 16 independent variables, this was an improvement of 5.35%. Final scoring on the rock genre received an F1 score of .7616, ROC-AUC score of .7753, and Accuracy score of .6808. Compared to the main features model, this was an improvement of 6.38%. Of the positive predictions of song genre, I was able to predict about 6% better with user-generated tags than the baseline.
Conclusion:
Using statistical methods, I found that the additional user-generated term tag features gave a significant improvement to the model classification of jazz and rock songs. Based on my findings, I would suggest to a psuedo-client that building out a social platform on top of an existing song recommendation platform would be beneficial for better classification of music by about 6% with the given data. I would expect more robust results with more advanced models that scaled with the size of the data. This was a preliminary analysis and there could be much greater value from offering social products in addition to the existing song recommendation product, solely because it provides and extra layer of depth to the data that can be used for other analyses.
Sources:
(1)
Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere.
The Million Song Dataset. In Proceedings of the 12th International Society
for Music Information Retrieval Conference (ISMIR 2011), 2011.

