
README
Evaluating benchmarks on MSR-VTT dataset.
What is this repository for?
Cotains code for end-to-end video captioning system which  makes use of MST-VTT dataset/
How do I get set up?
Requirements:
a) Keras 
b) Tensorflow 
c) ffmpeg 
d) OpenCV 
e) imageio 
f) sklearn 
g) nltk 
For Meteor Metric:
Download tar file from http://www.cs.cmu.edu/~alavie/METEOR/download/meteor-1.5.tar.gz
Copy meteor-1.5.jar file from there (It is used to run the meteor metric)
Details
Dataset size is about: 20 GB (downloaded using the videodatainfo_2017.json)
Downloading on 16 processors takes 5-6 hours.
Extracting frames from videos takes about 6 hours running 16 jobs at a time.
Video Feature Extraction using Tesla K-80 GPU takes about 10 hours. (uses pre-trained Imagenet to extract features).
Text feeatures (captions) are represented as one-hot-encoding - with a vocab size of about 30,000 words.
The above text features then converted into a word embedding of size 500.
The above video and text features are merged and passed into am LSTM network and is trained to minimize cross entropy loss.
Who do I talk to?
For any doubts:
Contact:
Chandra S Narain Kappera [ck2840@columbia.edu]

