
Sign Language Recognition System with Probabilistic Models
The overall goal of this project is to familiarize myself with Probabilistic Models by building a word recognizer for American Sign Language video sequences.  In particular, this project employs  hidden Markov models (HMM's) to analyze a series of measurements taken from videos of American Sign Language (ASL) collected for research (see the RWTH-BOSTON-104 Database).  In this video, the right-hand x and y locations are plotted as the speaker signs the sentence.

The raw data, train, and test sets are pre-defined.  We derive a variety of feature sets (explored in Part 1), as well as implement three different model selection criterion to determine the optimal number of hidden states for each word model (explored in Part 2). Finally, in Part 3 we implement the recognizer and compare the effects the different combinations of feature sets and model selection criteria.
References

Junichi Yamagishi - An Introduction to HMM-Based Speech Synthesis
Heiga Zen - Deep Learning in Speech Synthesis
DeepMind - Wavenet

Provided Raw Data
The data in the asl_recognizer/data/ directory was derived from
the RWTH-BOSTON-104 Database.
The handpositions (hand_condensed.csv) are pulled directly from
the database boston104.handpositions.rybach-forster-dreuw-2009-09-25.full.xml. The three markers are:

0  speaker's left hand
1  speaker's right hand
2  speaker's nose
X and Y values of the video frame increase left to right and top to bottom.

Take a look at the sample ASL recognizer video
to see how the hand locations are tracked.
The videos are sentences with translations provided in the database.
For purposes of this project, the sentences have been pre-segmented into words
based on slow motion examination of the files.
These segments are provided in the train_words.csv and test_words.csv files
in the form of start and end frames (inclusive).
The videos in the corpus include recordings from three different ASL speakers.
The mappings for the three speakers to video are included in the speaker.csv
file.
Install
This project requires Python 3 and the following Python libraries installed:

NumPy
SciPy
scikit-learn
pandas
matplotlib
jupyter
hmmlearn

Notes:

It is highly recommended that you install the Anaconda distribution of Python.
The most recent development version of hmmlearn, 0.2.1, contains a bugfix related to the log function, which is used in this project.  In order to install this version of hmmearn, install it directly from its repo with the following command from within your activated Anaconda environment:

pip install git+https://github.com/hmmlearn/hmmlearn.git
Run
In a terminal or command window, navigate to the top-level project directory (that contains this README) and run one of the following command:
jupyter notebook asl_recognizer.ipynb
This will open the Jupyter Notebook software and notebook in your browser which is where you will directly edit and run your code.

